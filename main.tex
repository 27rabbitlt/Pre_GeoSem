%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Welcome to Overleaf --- just edit your LaTeX on the left,
% and we'll compile it for you on the right. If you open the
% 'Share' menu, you can invite other users to edit at the same
% time. See www.overleaf.com/learn for more info. Enjoy!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows.meta,calc,intersections}
\usetheme{Copenhagen}
\usecolortheme{default}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{amsmath}

\bibliographystyle{plain} % or any other style
\newcommand{\supercite}[1]{\textsuperscript{\cite{#1}}}


\setbeamerfont{footnote}{size=\tiny}

\newcommand{\lf}{\left[}
\newcommand{\rf}{\right]}
\newcommand{\ld}{\left{}
\newcommand{\rd}{\right}}
\newcommand{\lx}{\left(}
\newcommand{\rx}{\right)}

\newcommand{\red}[1]{\textcolor{red}{#1}}

% \title[Seminar Geometry: Combinatorics and Algorithms: Small-dimensional linear programming and convex hulls made easy] %optional
% {Seminar Geometry: Combinatorics and Algorithms: Small-dimensional linear programming and convex hulls made easy}

\title[Seminar Geometry: Combinatorics and Algorithms FS25] %页脚
{Small-dimensional Linear Programming and Convex Hulls Made Easy}
\subtitle{Raimund Seidel}


% \subtitle{Demonstrating the Copenhagen theme}
\author[Teng Liu] % (optional)
{Teng Liu}

% \institute[VFU] % (optional)
% {
%   \inst{1}%
%   Faculty of Physics\\
%   Very Famous University
%   \and
%   \inst{2}%
%   Faculty of Chemistry\\
%   Very Famous University
% }

\date[]{April 3rd 2025}

% Use a simple TikZ graphic to show where the logo is positioned
% \logo{\begin{tikzpicture}
% \filldraw[color=red!50, fill=red!25, very thick](0,0) circle (0.5);
% \node[draw,color=white] at (0,0) {LOGO HERE};
% \end{tikzpicture}}

%End of title page configuration block
%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------
\begin{document}
\frame{\titlepage}
%---------------------------------------------------------

\begin{frame}{Background}
	Linear programming captures one of the most canonical and influential constrained optimization problems. \\~\

	More precisely, it asks to maximize or minimize a linear objective under linear
	inequality and equality constraints.

	% \begin{figure}[ht]
	%     \centering
	%     \includegraphics[width=.4\linewidth]{pre_1_1.png}
	%     \caption{Five random points on a plane}
	%     \label{fig11}
	% \end{figure}

	\begin{align*}
		\max / \min \; c^T         & x       \\
		\text{subject to} \;\;\; A & x \le e \\
		B                          & x \ge f \\
		C                          & x = g
	\end{align*}

	% this can be interpreted as maximizing profits we get from manufacturing some products with prices respectively and a certain budgets e

\end{frame}

%%% Simplex Method 1
\begin{frame}{Background - Geometrical Interpretation}
	Every constraint corresponds to a hyperplane (a halfspace, precisely). The intersection of several halfspaces gives us a polyhedron.
	\begin{center}
		\begin{tikzpicture}[scale=0.8]

			% Axes
			\draw[->] (-1,0) -- (5.5,0) node[below] {$x$};
			\draw[->] (0,-1) -- (0,5.5) node[left] {$y$};

			% Constraints as lines
			\draw[thick, name path=c1] (0,3) -- (3,0) node[pos=0.6, above left] {\small $x + y \le 3$};

			% Feasible region (polygon)
			\fill[blue!20, opacity=0.5]
			(0,0) --
			(3, 0) coordinate (opt) --
			(0,3) --
			cycle;

			% Mark the feasible region
			\draw[thick, blue] (0,0) -- (3, 0) -- (0, 3) -- cycle;

			% Objective function direction (e.g., maximize cᵗx = 4x + y)
			\draw[->, red, thick] (3,0) -- ++(1,0.25) node[above right] {\small $\max 4x + y$};

			\draw[thick, red!60] (2,4) -- (3.25,-1) node [ right] {\small $4x+y=12$ };

			% Optimal point
			\filldraw[red] (3,0) circle (2pt) node[above left] {Optimal};

		\end{tikzpicture}
	\end{center}
\end{frame}

\begin{frame}
	\begin{center}
		\begin{tikzpicture}[scale=1.5]
			% Axes
			\draw[->] (-2,0) -- (2,0) node[right] {$x_1$};
			\draw[->] (0,-2) -- (0,2) node[above] {$x_2$};

			% Hyperplane (line: x1 + x2 = 0)
			\draw[thick, black] (-2,2) -- (2,-2) node[below left] {Hyperplane: $x_1 + x_2 = 0$};
			\node at (1.5, 1.5) {Halfspace: $x_1 + x_2 \ge 0$};

			\filldraw[green, opacity=0.5] (-2, 2) -- (2, -2) -- (3, -1) -- (-1, 3) -- cycle;

			% \filldraw[blue, opacity=0.5] (-2, 2) -- (2, -2) -- (1, -3) -- (-3, 1) -- cycle;
			% Labels
		\end{tikzpicture}
	\end{center}
\end{frame}
\begin{frame}{Background - Simplex Method}
	%% Simplex method is an impressive algorithm that solves lp in an intuitive way.
	%% 
	\begin{exampleblock}{Fact}
		Optimal solution can always be achieved by some vertex.
	\end{exampleblock}

	\begin{exampleblock}{Fact}
		If there exists any vertex with better objective value than current vertex, we always have a neighbour vertex with better value.
	\end{exampleblock}

	\pause

	Firstly select any starting vertex. \\~\

	Each step in Simplex Method move from one vertex to its neighbour with larger objective.

	% If we currently we are not in optimal vertex, there are always a neighbour vertex with better objective.
	% This can be proved intuitively with this sketch: let's assume the feasible solution (the polyhedron) is full-dimensioned.
	% Then for any vertex, if we construct a ball centering at that vertex, the intersection of ball and polyhedra
	% would be also full-dimensioned.
	% We only need to show if we take any vertex as the origin and consider the ray formed by origin and its neighbour,
	% it gives a cone, identical to the one formed by considering the intersection of all hyperplane incident to origin.

\end{frame}

%%% Simplex Method 2

\begin{frame}{Background - Simplex Method}
	Firstly select any starting vertex. \\~\

	Each step in Simplex Method move from one vertex to its neighbour with larger objective.

	\begin{figure}[r]
		\includegraphics[width=0.4\linewidth]{pics/simplex_method.png}
		% \caption{}
		% \label{figsmallcircle}
	\end{figure}

	% If we currently we are not in optimal vertex, there are always a neighbour vertex with better objective.
	% This can be proved intuitively with this sketch: let's assume the feasible solution (the polyhedron) is full-dimensioned.
	% Then for any vertex, if we construct a ball centering at that vertex, the intersection of ball and polyhedra
	% would be also full-dimensioned.
	% We only need to show if we take any vertex as the origin and consider the ray formed by origin and its neighbour,
	% it gives a cone, identical to the one formed by considering the intersection of all hyperplane incident to origin.

\end{frame}

\begin{frame}{Background - Simplex Method Running Time}

	\begin{table}[h]
		\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\textbf{Algorithm}      & \textbf{Complexity} & \textbf{Speed} & \textbf{Path}        \\ \hline
			\textbf{Simplex}        & Exponential         & Fast           & Edges of polyhedron  \\ \hline
			\textbf{Ellipsoid}      & Polynomial          & Slow           & Ellipsoid shrinking  \\ \hline
			\textbf{Interior Point} & Polynomial          & Fast           & Through the interior \\ \hline
		\end{tabular}
		\caption{Comparison of Famous LP Algorithms}
	\end{table}

	% Even though we have Interior Method and Ellipsoid Method, the most widely used algorithm is still Simplex Method
	% due to its efficiency in practice.
	% If we currently we are not in optimal vertex, there are always a neighbour vertex with better objective.
	% This can be proved intuitively with this sketch: let's assume the feasible solution (the polyhedron) is full-dimensioned.
	% Then for any vertex, if we construct a ball centering at that vertex, the intersection of ball and polyhedra
	% would be also full-dimensioned.
	% We only need to show if we take any vertex as the origin and consider the ray formed by origin and its neighbour,
	% it gives a cone, identical to the one formed by considering the intersection of all hyperplane incident to origin.

\end{frame}
\begin{frame}{Previous Work}
	When $d$, the dimension of LP (i.e. the number of variables), is considered to be constant,
	then LP can be solved in time linear in $m$, the number of constraints.

	\vspace{\baselineskip}

	These methods\supercite{M2} are complicated and running time superexponential to $d$: e.g. $3^{d^2}$.

	\vspace{\baselineskip}

	We also have a simple algorithm\supercite{C2} with complexity $O(d^2m) + (\log m)O(d)^{d/2 + O(1)} + O(d^4 \sqrt{m} \log m)$
	but the analysis is involved.

	\vspace{\baselineskip}

	\textbf{The presented algorithm gives a simple procedure and complexity $O(d!m)$.}
\end{frame}

\begin{frame}{Algorithm - Sketch}

	This algorithm makes use of a very simple idea:

	\vspace{\baselineskip}

	Suppose we delete one constraint $\vec{a} x \ge b$, and somehow managed to solve the new LP.
	Then we check if the optimum $x^*$ conforms with the deleted constraint.

	\vspace{\baselineskip}

	\begin{itemize}
		\item If it doesn't violate the constraint, i.e. $\vec{a} x^* \ge b$, then $x^*$ is also
		      the optimum for original LP, because deleting one constraint
		      gives us a larger feasible area.
		\item If it violates the constraint, then the opt value lies on the hyperplane
		      corresponding to this constraint.  We can project the feasible area
		      onto the corresponding hyperplane, thus reducing dimension by 1. Then
		      solve the new LP problem with lower dimension.
	\end{itemize}
\end{frame}

\begin{frame}{Algorithm - Correctness 1}
	Let original feasible region be $D$, feasible region after deleting constraint be $D'$, the optimum for $D'$ be $x^*$ .
	Clearly $D \subseteq D'$ and in this case $x^* \in D$.
	\begin{align*}
		u^T x^* \underset{\text{(i)}}{\le} \max_{D}  \underset{\text{(ii)}}{\le} \max_{D'} = u^T x^*
	\end{align*}

	(i): $x^* \in D$, by definition of $\max$.

	\vspace{\baselineskip}

	(ii): Optimizing on a larger area gives no worse optimal value than the original one.
\end{frame}

\begin{frame}{Algorithm - Correctness 2}
	% Let original feasible region be $D$, feasible region after deleting bounding hyperplane $h$ be $D'$, the optimum for $D'$ be $x^*$ .
	% In this case $x^* \notin D$, then $x^* \in h$.

	% \vspace{\baselineskip}
	\begin{center}
		\begin{tikzpicture}
			% axes 
			\draw[->] (-0.5, 0) -- (5, 0) node[below] {$x$};
			\draw[->] (0, -0.5) -- (0, 5) node[left] {$y$};
			% Define two paths
			% \path[name path=a] (1, 0) -- (1, 7);
			% \path[name path=b] (0, 1) -- (7, 1);
			% \path[name path=c] (4, 0) -- (4, 7);
			% \path[name path=d] (5, 0) -- (0, 5);
			% \path[name path=e] (0, 1) -- (5, 6);
			%
			% \path[name intersections={of=a and e, by=A}];
			% \path[name intersections={of=b and a, by=B}];
			% \path[name intersections={of=c and b, by=C}];
			% \path[name intersections={of=d and c, by=D}];
			% \path[name intersections={of=e and d, by=E}];
			% Calculate intersection
			\coordinate (A) at (1, 1);
			\coordinate (B) at (4, 1);
			\coordinate (C) at (4, 3);
			\coordinate (D) at (2, 4);
			\coordinate (E) at (1, 3);
			\coordinate (F) at (4, 6);

			\fill[blue!60] (A) -- (B) -- (C) -- (D) -- (E) --cycle;
			\fill[green!20] (C) -- (F) -- (D) --cycle;

			% Draw the intersection point
			\draw[thick, blue] (A) -- (B);
			\draw[thick, blue] (B) -- (C);
			\draw[thick, blue] (C) -- (D);
			\draw[thick] ($ (C)!-0.2!(D) $) -- ($ (C)!1.2!(D) $) node [left] {$h$};
			\draw[thick, blue] (D) -- (E);
			\draw[thick, blue] (E) -- (A);

			\coordinate (X) at (3, 2);
			\node[circle, label=below:X, fill=black, inner sep=1pt] at (X) {};
			\coordinate (X') at (4, 6);
			\node[circle, label=above:X', fill=black, inner sep=1pt] at (X') {};

			\node at (2, 2) {$D$};
			\node at (3, 4.5) {$D'$};

			\draw[thick, black] (X) -- (X');

			\node [right] at (5, 4) {A better point lies on $h$};
			\draw[->, red] (5, 4) -- (3.7, 3.4);
		\end{tikzpicture}
	\end{center}

	% This is because if we consider the points along the line connecting
	% new LP's opt point and original LP's opt point, by convexity we have that
	% we have a better point on the hyperplane.
	% This is also not bad, at least we decrease the dimension (we restrict the problem
	% on a hyperplane, which has dimension $d - 1$), which gives us the intuition to recurssively analyse.
\end{frame}

\begin{frame}{Algorithm - Further Problems}
	Some problems left to deal with:

	\begin{itemize}
		\item What if after deletion of bounding hyperplane the new problem become unbounded?
		\item When to stop the recurssion?
		\item What's next when we reduce the dimension?
	\end{itemize}
\end{frame}
\begin{frame}{Algorithm - Unboundedness and Recurssion}
	The first two problems can be solved by adding a bounding box, i.e. explicit lower and upper bounds for
	variables $-\alpha \le x_i \le \alpha$ for all $d$ variables $x_i$.

	\vspace{\baselineskip}

	Recurssion stops when all original hyperplanes are deleted and it's impossible to have unbounded case.

\end{frame}
\begin{frame}{Algorithm - Reducing Dimension}
	Denote the set of $m$ halfspaces as $\mathcal{H}$ and the hyperplane corresponding to the deleted halfspace as $h$.
	Let $u$ be the optimizing direction and $v$ stands for the optimum point for original LP problem.

	\vspace{\baselineskip}

	If $\bar{u}$ is the projection of $u$ on $h$ and $\bar{\mathcal{H}} \coloneq \{H \bigcap h \mid H \in \mathcal{H}\}$, then $v$ is exactly
	the optimum point for LP problem with constraints $\bar{\mathcal{H}}$ and optimizing direction $\bar{u}$.
\end{frame}
\begin{frame}{Algorithm - Time Complexity}
	When $d = 1$, problem can be solved in $O(m)$.

	\vspace{\baselineskip}

	When $m = 1$, problem can be solved in $O(d)$.

	\vspace{\baselineskip}

	The interesting part is when we delete a constraint and the optimum point changes.
	Since original optimum point $x$ has exactly $d$ tight constraints (i.e.
	$x$ is on exactly $d$ hyperplanes) then deleting a constraint uniformly randomly ensures
	the probability of $x'$ differs from $x$ no more than $d / m$.
\end{frame}
\begin{frame}{Algorithm - Time Complexity}
	When $d = 1$, problem can be solved in $O(m)$.

	\vspace{\baselineskip}

	When $m = 1$, problem can be solved in $O(d)$.

	\vspace{\baselineskip}

	The interesting part is when we delete a constraint and the optimum point changes.
	\red{Since original optimum point} $\red{x}$ \red{has exactly} $\red{d}$ \red{tight constraints} (i.e.
	$x$ is on exactly $d$ hyperplanes) then deleting a constraint uniformly randomly ensures
	the probability of $x'$ differs from $x$ no more than $d / m$.
\end{frame}

\begin{frame}{Algorithm - Time Complexity}
	\tdplotsetmaincoords{60}{120}

	\begin{tikzpicture}[tdplot_main_coords, scale=3]

		% Base square vertices
		\coordinate (A) at (1, 1, 0);
		\coordinate (B) at (-1, 1, 0);
		\coordinate (C) at (-1, -1, 0);
		\coordinate (D) at (1, -1, 0);

		% Tip of the cone
		\coordinate (O) at (0, 0, 1.5);

		% Draw base
		% \draw[thick] (A) -- (B) -- (C) -- (D) -- cycle;

		% Draw sides
		\draw[fill=gray!30, opacity=0.8] (O) -- (B) -- (C) -- cycle;
		\draw[fill=gray!30, opacity=0.8] (O) -- (C) -- (D) -- cycle;
		\draw[fill=gray!30, opacity=0.8] (O) -- (D) -- (A) -- cycle;
		\draw[fill=gray!30, opacity=0.8] (O) -- (A) -- (B) -- cycle;

        \filldraw[fill=red] (O) circle (2pt);


	\end{tikzpicture}

\end{frame}

\begin{frame}{Algorithm - Time Complexity}
    Even if there are $k > d$ constraints are tight at $x$, the optimum point for original LP, at most $d$ of them 
    has the property that deleting it leads a better optimum.

    
    \vspace{\baselineskip}
    
    \begin{align*}
        \exists v, \;\; &a_1^Tv > 0 \\
                   &a_2^Tv \le 0 \\ 
                   &\cdots \\
                   &a_k^Tv \le 0
    \end{align*}
\end{frame}
\begin{frame}{Algorithm - Time Complexity}
	\[
		T(d, m) \le
		\begin{cases}
			O(m)                         & \text{if } d = 1, \\
			O(d)                         & \text{if } m = 1, \\
			T(d, m - 1) + O(d) + \dfrac{d}{m} O(dm) +        \\
			\dfrac{d}{m} T(d - 1, m - 1) & \text{otherwise}.
		\end{cases}
	\]

	Expand everything and we get:

	\begin{align*}
		T(d, m) = O(\sum_{1\le i \le d} \frac{i^2}{i!} d! m) = O(d! m)
	\end{align*}

\end{frame}
\begin{frame}{Backward Analysis}
	Sometimes it's hard to determine the probability if thinking \textit{forward}, while surprisingly straightforward if thinking \textit{backward}. \\~\

	\begin{block}{Smallest Enclosing Circle}
		Given a set of points $P$ in Euclidean plane, compute the smallest circle that contains them all.
	\end{block}

	\begin{figure}[r]
		\includegraphics[width=0.4\linewidth]{pics/smallest_enclosing_circle_illustration.png}
		% \caption{}
		% \label{figsmallcircle}
	\end{figure}

	% Given first $i$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$ \\~\

	% Given first $i+1$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$

	% in fact, the two algorithms shares a trick in analysing some algos in common, it's called backward analysis by Seidel. the details are in depth explained in another paper where the author listed several other algorithms that backward analysis can also apply, either to give a simple proof for a previously complicated algorithm or a completely new, yet still simple proof for a new algorithm.

	% the first one is difficult to calculate because it's intrinsicly difficult. The probability differs when we have different set of first i points. So it's indeed hard to give a probability for any set of first i points, but we don't care about the probability for any specific set of first i points whatsoever: we care about only the expectation: as long as the expectation of this probability among all possible sets of first i points is bounded by some value then we are happy.

	% the second one is smarter in the sense that it doesn't depend on the set of i+1 points. whichever we choose we always have the same probability

	% here we need two pictures illustrating this:

\end{frame}

\begin{frame}{Backward Analysis - Smallest Enclosing Circle}

	Emo Welzl gave an efficient random algorithm with surprisingly simple procedure:

	\begin{figure}[h]
		\includegraphics[width=1.0\linewidth]{pics/smallest_enclosing_circle_psudocode.png}
		% \caption{Convex hull structure: 5, 4, 3, 2.  There are 73 different structures for $n=17$.}
		% \label{figstruct}
	\end{figure}

	% Given first $i$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$ \\~\

	% Given first $i+1$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$

	% in fact, the two algorithms shares a trick in analysing some algos in common, it's called backward analysis by Seidel. the details are in depth explained in another paper where the author listed several other algorithms that backward analysis can also apply, either to give a simple proof for a previously complicated algorithm or a completely new, yet still simple proof for a new algorithm.

	% the first one is difficult to calculate because it's intrinsicly difficult. The probability differs when we have different set of first i points. So it's indeed hard to give a probability for any set of first i points, but we don't care about the probability for any specific set of first i points whatsoever: we care about only the expectation: as long as the expectation of this probability among all possible sets of first i points is bounded by some value then we are happy.

	% the second one is smarter in the sense that it doesn't depend on the set of i+1 points. whichever we choose we always have the same probability

	% here we need two pictures illustrating this:

\end{frame}

\begin{frame}
	\tdplotsetmaincoords{70}{110} % view angles

	\begin{center}
		\begin{tikzpicture}[tdplot_main_coords, scale=2]

			% Axes
			\draw[->] (0,0,0) -- (2,0,0) node[anchor=north east]{$x$};
			\draw[->] (0,0,0) -- (0,2,0) node[anchor=north west]{$y$};
			\draw[->] (0,0,0) -- (0,0,2) node[anchor=south]{$z$};

			% Hyperplane: e.g. x + y + z = 1 ⇒ plot the triangle (1,0,0), (0,1,0), (0,0,1)
			\filldraw[fill=red!30, opacity=0.5] (1,0,0) -- (0,1,0) -- (0,0,1) -- cycle;
			\draw[thick, red] (1,0,0) -- (0,1,0) -- (0,0,1) -- (1,0,0);

			% Label for the plane
			\node at (0.4,0.4,0.4) {\scriptsize Hyperplane: $x + y + z = 1$};

		\end{tikzpicture}
	\end{center}
\end{frame}

\begin{frame}[allowframebreaks]{References}
	\bibliography{ref}
\end{frame}
\end{document}
