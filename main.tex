%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Welcome to Overleaf --- just edit your LaTeX on the left,
% and we'll compile it for you on the right. If you open the
% 'Share' menu, you can invite other users to edit at the same
% time. See www.overleaf.com/learn for more info. Enjoy!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{beamer}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetheme{Copenhagen}
\usecolortheme{default}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{svg}

\setbeamerfont{footnote}{size=\tiny}

\newcommand{\lf}{\left[}
\newcommand{\rf}{\right]}
\newcommand{\ld}{\left{}
\newcommand{\rd}{\right}}
\newcommand{\lx}{\left(}
\newcommand{\rx}{\right)}


% \title[Seminar Geometry: Combinatorics and Algorithms: Small-dimensional linear programming and convex hulls made easy] %optional
% {Seminar Geometry: Combinatorics and Algorithms: Small-dimensional linear programming and convex hulls made easy}

\title[Seminar Geometry: Combinatorics and Algorithms FS25] %页脚
{Small-dimensional Linear Programming and Convex Hulls Made Easy}
\subtitle{Raimund Seidel}


% \subtitle{Demonstrating the Copenhagen theme}
\author[Teng Liu] % (optional)
{Teng Liu}

% \institute[VFU] % (optional)
% {
%   \inst{1}%
%   Faculty of Physics\\
%   Very Famous University
%   \and
%   \inst{2}%
%   Faculty of Chemistry\\
%   Very Famous University
% }

\date[]{April 3rd 2025}

% Use a simple TikZ graphic to show where the logo is positioned
% \logo{\begin{tikzpicture}
% \filldraw[color=red!50, fill=red!25, very thick](0,0) circle (0.5);
% \node[draw,color=white] at (0,0) {LOGO HERE};
% \end{tikzpicture}}

%End of title page configuration block
%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------
\begin{document}
\frame{\titlepage}
%---------------------------------------------------------

\begin{frame}{Background}
	Linear programming captures one of the most canonical and influential constrained optimization problems. \\~\

	More precisely, it asks to maximize or minimize a linear objective under linear
	inequality and equality constraints.

	% \begin{figure}[ht]
	%     \centering
	%     \includegraphics[width=.4\linewidth]{pre_1_1.png}
	%     \caption{Five random points on a plane}
	%     \label{fig11}
	% \end{figure}

	\begin{align*}
		\max / \min \; c^T         & x       \\
		\text{subject to} \;\;\; A & x \le e \\
		B                          & x \ge f \\
		C                          & x = g
	\end{align*}

	% this can be interpreted as maximizing profits we get from manufacturing some products with prices respectively and a certain budgets e

\end{frame}

%%% Simplex Method 1
\begin{frame}{Background - Geometrical Interpretation}
	Every constraint corresponds to a hyperplane (a halfspace, precisely).

	The intersection of several halfspaces gives us a polyhedron.

	We tend to optimize over this feasible area.
\end{frame}

\begin{frame}
	\begin{center}
		\begin{tikzpicture}[scale=1.5]
			% Axes
			\draw[->] (-2,0) -- (2,0) node[right] {$x_1$};
			\draw[->] (0,-2) -- (0,2) node[above] {$x_2$};

			% Hyperplane (line: x1 + x2 = 0)
			\draw[thick, black] (-2,2) -- (2,-2) node[below left] {Hyperplane: $x_1 + x_2 = 0$};
            \node at (1.5, 1.5) {Halfspace: $x_1 + x_2 \ge 0$}

            \filldraw[green, opacity=0.5] (-2, 2) -- (2, -2) -- (3, -1) -- (-1, 3) -- cycle;

            % \filldraw[blue, opacity=0.5] (-2, 2) -- (2, -2) -- (1, -3) -- (-3, 1) -- cycle;
			% Labels
		\end{tikzpicture}
	\end{center}
\end{frame}
\begin{frame}{Background - Simplex Method}
	%% Simplex method is an impressive algorithm that solves lp in an intuitive way.
	%% 
	\begin{exampleblock}{Fact}
		Optimal solution can always be achieved by some vertex.
	\end{exampleblock}

	\begin{exampleblock}{Fact}
		If there exists any vertex with better objective value than current vertex, we always have a neighbour vertex with better value.
	\end{exampleblock}

	Firstly select any starting vertex. \\~\

	Each step in Simplex Method move from one vertex to its neighbour with larger objective.

	% If we currently we are not in optimal vertex, there are always a neighbour vertex with better objective.
	% This can be proved intuitively with this sketch: let's assume the feasible solution (the polyhedron) is full-dimensioned.
	% Then for any vertex, if we construct a ball centering at that vertex, the intersection of ball and polyhedra
	% would be also full-dimensioned.
	% We only need to show if we take any vertex as the origin and consider the ray formed by origin and its neighbour,
	% it gives a cone, identical to the one formed by considering the intersection of all hyperplane incident to origin.

\end{frame}

%%% Simplex Method 2

\begin{frame}{Background - Simplex Method}
	Firstly select any starting vertex. \\~\

	Each step in Simplex Method move from one vertex to its neighbour with larger objective.

	\begin{figure}[r]
		\includegraphics[width=0.4\linewidth]{pics/simplex_method.png}
		% \caption{}
		% \label{figsmallcircle}
	\end{figure}

	% If we currently we are not in optimal vertex, there are always a neighbour vertex with better objective.
	% This can be proved intuitively with this sketch: let's assume the feasible solution (the polyhedron) is full-dimensioned.
	% Then for any vertex, if we construct a ball centering at that vertex, the intersection of ball and polyhedra
	% would be also full-dimensioned.
	% We only need to show if we take any vertex as the origin and consider the ray formed by origin and its neighbour,
	% it gives a cone, identical to the one formed by considering the intersection of all hyperplane incident to origin.

\end{frame}

\begin{frame}{Background - Simplex Method Running Time}

	Add here the complexity and running time tables for three methods. \\~\

	The three most famous algorithms for LP: Interior Method, Ellipsoid Method, Simplex Method.

	Even though we have Interior Method and Ellipsoid Method, the most widely used algorithm is still Simplex Method
	due to its efficiency in practice.
	% If we currently we are not in optimal vertex, there are always a neighbour vertex with better objective.
	% This can be proved intuitively with this sketch: let's assume the feasible solution (the polyhedron) is full-dimensioned.
	% Then for any vertex, if we construct a ball centering at that vertex, the intersection of ball and polyhedra
	% would be also full-dimensioned.
	% We only need to show if we take any vertex as the origin and consider the ray formed by origin and its neighbour,
	% it gives a cone, identical to the one formed by considering the intersection of all hyperplane incident to origin.

\end{frame}

\begin{frame}{Previous Work}
	there are some previous work showing that
	if $d$, the dimension or i.e. the number of variables in LP, is considered as a constant,
	then LP can be solved in time linear to $m$, the number of constraints.

	However these methods are: complicated; and running time superexponential to $d$: e.g. $3^{d^2}$.

	we also have a simple algorithm with complexity $O(d^2m) + (\log m)O(d)^{d/2 + O(1)} + O(d^4 \sqrt{m} \log m)$
	but the analysis is involved.

	The presented algorithm gives a simple procedure and complexity $O(d!m)$.

\end{frame}

\begin{frame}
	We were talking about other algorithms for LP but what do these have to do with this paper?

	and we talked so much about simplex method.

	But unfortunately it has nothing to do with what we are going to present.

	This algorithm makes use of a very simple idea:

	Suppose we delete one constraint, and somehow managed to solve the new LP.
	Then we check if the opt position conforms with the deleted constraint.

	If it doesn't violate the constraint, then we are done. (this is obvious,
	since we have a larger feasible area, we certainly have better opt value)

	If it violates the constraint, then the opt value lies on the hyperplane
	corresponding to this constraint.
	This is because if we consider the points along the line connecting
	new LP's opt point and original LP's opt point, by convexity we have that
	we have a better point on the hyperplane.
	This is also not bad, at least we decrease the dimension (we restrict the problem
	on a hyperplane, which has dimension $d - 1$), which gives us the intuition to recurssively analyse.

	Reducing dimension is certainly something good for computation but how much cost it would take to "transform"?

	Let's analyse the complexity of this step.

\end{frame}

\begin{frame}{Backward Analysis}
	Sometimes it's hard to determine the probability if thinking \textit{forward}, while surprisingly straightforward if thinking \textit{backward}. \\~\

	\begin{block}{Smallest Enclosing Circle}
		Given a set of points $P$ in Euclidean plane, compute the smallest circle that contains them all.
	\end{block}

	\begin{figure}[r]
		\includegraphics[width=0.4\linewidth]{pics/smallest_enclosing_circle_illustration.png}
		% \caption{}
		% \label{figsmallcircle}
	\end{figure}

	% Given first $i$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$ \\~\

	% Given first $i+1$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$

	% in fact, the two algorithms shares a trick in analysing some algos in common, it's called backward analysis by Seidel. the details are in depth explained in another paper where the author listed several other algorithms that backward analysis can also apply, either to give a simple proof for a previously complicated algorithm or a completely new, yet still simple proof for a new algorithm.

	% the first one is difficult to calculate because it's intrinsicly difficult. The probability differs when we have different set of first i points. So it's indeed hard to give a probability for any set of first i points, but we don't care about the probability for any specific set of first i points whatsoever: we care about only the expectation: as long as the expectation of this probability among all possible sets of first i points is bounded by some value then we are happy.

	% the second one is smarter in the sense that it doesn't depend on the set of i+1 points. whichever we choose we always have the same probability

	% here we need two pictures illustrating this:

\end{frame}

\begin{frame}{Backward Analysis - Smallest Enclosing Circle}

	Emo Welzl gave an efficient random algorithm with surprisingly simple procedure:

	\begin{figure}[h]
		\includegraphics[width=1.0\linewidth]{pics/smallest_enclosing_circle_psudocode.png}
		% \caption{Convex hull structure: 5, 4, 3, 2.  There are 73 different structures for $n=17$.}
		% \label{figstruct}
	\end{figure}

	% Given first $i$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$ \\~\

	% Given first $i+1$ points, $\text{Pr}\lf \text{No.}(i+1) \text{ point falls in the interior of circle } C_{i+1} \rf = \;?$

	% in fact, the two algorithms shares a trick in analysing some algos in common, it's called backward analysis by Seidel. the details are in depth explained in another paper where the author listed several other algorithms that backward analysis can also apply, either to give a simple proof for a previously complicated algorithm or a completely new, yet still simple proof for a new algorithm.

	% the first one is difficult to calculate because it's intrinsicly difficult. The probability differs when we have different set of first i points. So it's indeed hard to give a probability for any set of first i points, but we don't care about the probability for any specific set of first i points whatsoever: we care about only the expectation: as long as the expectation of this probability among all possible sets of first i points is bounded by some value then we are happy.

	% the second one is smarter in the sense that it doesn't depend on the set of i+1 points. whichever we choose we always have the same probability

	% here we need two pictures illustrating this:

\end{frame}

\begin{frame}
    \tdplotsetmaincoords{70}{110} % view angles

\begin{center}
\begin{tikzpicture}[tdplot_main_coords, scale=2]

  % Axes
  \draw[->] (0,0,0) -- (2,0,0) node[anchor=north east]{$x$};
  \draw[->] (0,0,0) -- (0,2,0) node[anchor=north west]{$y$};
  \draw[->] (0,0,0) -- (0,0,2) node[anchor=south]{$z$};

  % Hyperplane: e.g. x + y + z = 1 ⇒ plot the triangle (1,0,0), (0,1,0), (0,0,1)
  \filldraw[fill=red!30, opacity=0.5] (1,0,0) -- (0,1,0) -- (0,0,1) -- cycle;
  \draw[thick, red] (1,0,0) -- (0,1,0) -- (0,0,1) -- (1,0,0);

  % Label for the plane
  \node at (0.4,0.4,0.4) {\scriptsize Hyperplane: $x + y + z = 1$};

\end{tikzpicture}
\end{center}
\end{frame}
\end{document}
